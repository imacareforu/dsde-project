import streamlit as st
from streamlit_folium import folium_static
import folium
import pandas as pd
import pydeck as pdk
import geopandas as gpd
import plotly.express as px
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from pymongo import MongoClient
import numpy as np
import re
import requests
import json
from folium import Choropleth, GeoJson, GeoJsonTooltip
from datetime import datetime, timedelta
import streamlit.components.v1 as components

st.set_page_config(page_title="traffy fondu dataset analysis", layout="wide")
st.title('Traffy Fondu Dataset Analysis')

# Load and prepare data
@st.cache_data
def load_data():
    data = pd.read_csv('bangkok_traffy.csv')
    
    # Clean and prepare data
    data = data.dropna()
    return data

# Load data
data = load_data()

tab1, tab2 = st.tabs(["üìä Overall", "üìé Statistic"])

with tab1:
    # ----------------------------- ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ -----------------------------
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    total_issues = len(data)
    completed_issues = len(data[data['state'] == '‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô'])
    incomplete_issues = total_issues - completed_issues

    col1, col2, col3 = st.columns(3)
    col1.metric("üìå Total reported issues", f"{total_issues:,}")
    col2.metric("‚úÖ Issues Resolved", f"{completed_issues:,}")
    col3.metric("‚è≥ Unresolved Issues", f"{incomplete_issues:,}")


#---------------------------------other function-------------------------------
def haversine_np(lon1, lat1, lon2, lat2):
    R = 6371  # Earth radius in km
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1

    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2
    return R * 2 * np.arcsin(np.sqrt(a))

with tab2:
    #---------------------------------histogram------------------------------------
    st.subheader("Distribution of Issue Types")
    # 1. ‡∏•‡πâ‡∏≤‡∏á format: ‡πÄ‡∏≠‡∏≤ { } ‡∏≠‡∏≠‡∏Å ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢ comma
    data['type_list'] = data['type'].str.strip("{}").str.split(",")

    # 2. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô flat list ‡∏î‡πâ‡∏ß‡∏¢ explode()
    all_types = data.explode('type_list')['type_list'].str.strip()
    all_types = all_types.replace('', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó')

    # 3. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ type
    type_counts = all_types.value_counts().reset_index()
    type_counts.columns = ['type', 'count']

    # 4. ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡∏î‡πâ‡∏ß‡∏¢ Plotly
    fig = px.bar(type_counts, x='type', y='count', text='count')
    fig.update_traces(textposition='outside')
    fig.update_layout(xaxis_title='type', yaxis_title='number of issues')

    # 5. ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô Streamlit
    st.plotly_chart(fig)

    # ---------------------------- Histogram: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏Ç‡∏ï -------------------
    st.subheader("Distribution of Issues Across Districts")

    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏Ç‡∏ï (‡∏Å‡∏£‡∏≠‡∏á NaN ‡∏≠‡∏≠‡∏Å)
    district_counts = data['district'].dropna().value_counts().reset_index()
    district_counts.columns = ['district', 'count']

    # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏ó‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢ Plotly
    fig_district = px.bar(
        district_counts,
        x='district',
        y='count',
        text='count'
    )
    fig_district.update_traces(textposition='outside')
    fig_district.update_layout(
        xaxis_title='district',
        yaxis_title='number of issues',
        xaxis_tickangle=-45
    )

    # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÉ‡∏ô Streamlit
    st.plotly_chart(fig_district)

with tab1:
# ----------------------------- pie chart -----------------------------

    # ‡πÅ‡∏¢‡∏Å Top N ‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
    top_types = type_counts.head(5)
    others = pd.DataFrame([{
        'type': '‡∏≠‡∏∑‡πà‡∏ô‡πÜ',
        'count': type_counts['count'][5:].sum()
    }])

    # ‡∏£‡∏ß‡∏°‡∏Å‡∏•‡∏±‡∏ö
    type_counts_pie = pd.concat([top_types, others], ignore_index=True)

    # ‡∏ß‡∏≤‡∏î Pie Chart
    st.subheader("Proportion of Issue Types")
    fig_type_pie = px.pie(
        type_counts_pie,
        names='type',
        values='count'
    )
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ô‡∏Å‡∏£‡∏≤‡∏ü
    fig_type_pie.update_traces(
        textinfo='label+percent',  # ‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠ + ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå + ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô
        textposition='inside'
    )
    st.plotly_chart(fig_type_pie)

with tab2:
    # ----------------------------- ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô star ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î -----------------------------
    st.subheader("Top-Rated Organizations")

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    org_star_df = data[['organization', 'star']].dropna()
    org_star_df['main_org'] = org_star_df['organization'].apply(lambda x: x.split(',')[0].strip())
    org_summary = org_star_df.groupby('main_org')['star'].agg(['count', 'mean']).reset_index()
    org_summary = org_summary[org_summary['count'] >= 5]  # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏°‡∏≤‡∏Å‡∏û‡∏≠
    org_summary = org_summary.sort_values(by='mean', ascending=False).head(5)

    # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü
    fig_star_org = px.bar(
        org_summary,
        x='mean',
        y='main_org',
        orientation='h',
        text='mean',
        title='üèÜTop 5 Organizations (at least 5 reviews)',
        labels={'mean': 'average rating', 'main_org': 'organization'}
    )
    fig_star_org.update_layout(yaxis=dict(autorange="reversed"))
    fig_star_org.update_traces(texttemplate='%{text:.2f}', textposition='outside')

    # ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô Streamlit
    st.plotly_chart(fig_star_org, use_container_width=False)


# ---------------------------------- Before/After Gallery ----------------------------------
all_types = data.explode('type_list')['type_list'].str.strip().replace('', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó').dropna().unique()
all_types = sorted(all_types)

data['timestamp'] = pd.to_datetime(data['timestamp'], format='mixed', errors='coerce')
data['year'] = data['timestamp'].dt.year
data['month'] = data['timestamp'].dt.month

data = data.dropna(subset=['timestamp'])

# ----------------------------------- Sidebar ---------------------------------
now = datetime.now()

# Default values
default_year = now.year
default_month = now.month - 1 if now.month > 1 else 12
all_months = list(range(1, 13))
all_districts = sorted(data['district'].dropna().unique())
district_options = ["‡∏ó‡∏∏‡∏Å‡πÄ‡∏Ç‡∏ï (‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø)"] + all_districts
all_types_raw = data.explode('type_list')['type_list'].str.strip().replace('', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó')
all_types = sorted(all_types_raw.dropna().unique().tolist())
type_options = ["‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó"] + all_types

# Sidebar filters (no form)
st.sidebar.header('Options')

selected_year = st.sidebar.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏µ", sorted(data['year'].unique(), reverse=True), index=0)
default_month_index = all_months.index(default_month)
selected_month = st.sidebar.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô", all_months, index=default_month_index)
selected_districts = st.sidebar.multiselect("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏Ç‡∏ï", district_options, default=["‡∏ó‡∏∏‡∏Å‡πÄ‡∏Ç‡∏ï (‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø)"])
selected_types = st.sidebar.multiselect("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó", type_options, default=["‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó"])

show_schools = st.sidebar.checkbox("‡πÅ‡∏™‡∏î‡∏á‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡∏à‡∏∏‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤", value=False)
# ‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á ‚Üí ‡πÅ‡∏™‡∏î‡∏á slider ‡πÄ‡∏û‡∏¥‡πà‡∏°
radius_km = None  # default ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏¥‡∏î
if show_schools:
    radius_km = st.sidebar.slider("‡∏£‡∏±‡∏®‡∏°‡∏µ (‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£)", min_value=0.0, max_value=5.0, value=2.0, step=0.5)

# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
filtered_data = data[
    (data['year'] == selected_year) &
    (data['month'] == selected_month)
]

if "‡∏ó‡∏∏‡∏Å‡πÄ‡∏Ç‡∏ï (‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø)" not in selected_districts:
    filtered_data = filtered_data[filtered_data['district'].isin(selected_districts)]

if "‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó" not in selected_types:
    filtered_data = filtered_data.explode('type_list')
    filtered_data['type_list'] = filtered_data['type_list'].str.strip().replace('', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó')
    filtered_data = filtered_data[filtered_data['type_list'].isin(selected_types)]

# ‡∏ï‡∏±‡∏î‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà coords ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ comma ‡∏≠‡∏≠‡∏Å
filtered_data = filtered_data[
    filtered_data['coords'].notna() &
    filtered_data['coords'].str.contains(",", na=False)
]
# ‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô lon, lat ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
coords_extracted = filtered_data['coords'].str.split(",", expand=True)

# ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
if coords_extracted.shape[1] >= 2:
    filtered_data['lon'] = pd.to_numeric(coords_extracted.iloc[:, 0], errors='coerce')
    filtered_data['lat'] = pd.to_numeric(coords_extracted.iloc[:, 1], errors='coerce')

    # ‡∏ï‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡∏û‡∏¥‡∏Å‡∏±‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
    filtered_data = filtered_data.dropna(subset=['lon', 'lat'])
else:
    filtered_data['lon'] = None
    filtered_data['lat'] = None

filtered_data['photo_after'] = filtered_data['photo_after'].fillna("")

# Define color mapping for states
state_colors = {
    '‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô': [0, 255, 0, 255],       
    '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£': [255, 255, 0, 255],    
    '‡∏£‡∏≠‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á': [255, 0, 0, 255] 
}

# Map the 'state' column to corresponding colors
filtered_data['color'] = filtered_data['state'].map(state_colors)

# For issue reports
filtered_data['tooltip_html'] = (
    "<b>Ticket ID:</b> " + filtered_data['ticket_id'].astype(str) + "<br/>"
    "<b>State:</b> " + filtered_data['state'] + "<br/>"
    "<b>Type:</b> " + filtered_data['type'].str.strip("{}") + "<br/>"
    "<b>Comment:</b> " + filtered_data['comment'] + "<br/>"
    "<b>District:</b> " + filtered_data['district'] + "<br/>"
    "<b>Before:</b><br/><img src='" + filtered_data['photo'] + "' width='200px'><br/>"
    "<b>After:</b><br/><img src='" + filtered_data['photo_after'] + "' width='200px'>"
)

with tab1:

    st.subheader("Issue Reports Map")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Filtered Report Count", f"{len(filtered_data):,}")

    layer_reports = pdk.Layer(
        "ScatterplotLayer",
        data=filtered_data,
        get_position='[lon, lat]',
        get_radius=75,
        get_color='color',
        pickable=True
    )

    #---------------------------external data----------------------

    # Load geojson
    with open('school_in_bangkok.geojson', encoding='utf-8') as f:
        geojson_data = json.load(f)

    # Extract feature data
    features = geojson_data['features']

    # Extract data into list of dicts
    records = []
    for feature in features:
        prop = feature['properties']
        coords = feature['geometry']['coordinates']
        record = {
            'name': prop.get('name'),
            'lon': coords[0],
            'lat': coords[1]
        }
        records.append(record)

    # Convert to DataFrame
    df_geo = pd.DataFrame(records)

    df_geo['tooltip_html'] = (
        "<b>Name:</b> " + df_geo['name']
    )
    # Filter schools within 2 km of any issue report
    report_lats = filtered_data['lat'].values
    report_lons = filtered_data['lon'].values

    school_distances = []
    for _, row in df_geo.iterrows():
        if len(report_lats) > 0 and len(report_lons) > 0:
            dists = haversine_np(report_lons, report_lats, row['lon'], row['lat'])
            min_dist = dists.min()
        else:
            min_dist = None
        school_distances.append(min_dist)

    df_geo['min_dist_km'] = school_distances
    df_geo = df_geo.dropna(subset=['min_dist_km'])
    if radius_km is not None:
        df_geo = df_geo[df_geo['min_dist_km'] <= radius_km]

    layer_schools = pdk.Layer(
        "ScatterplotLayer",
        data=df_geo,
        get_position='[lon, lat]',
        get_radius=100,
        get_color=[255, 255, 255, 255],
        pickable=True
    )

    #---------------------------combine data--------------------
    tooltip = {
        "html": "{tooltip_html}",
        "style": {"backgroundColor": "white", "color": "black"}
    }

    # Use either view_state, depending on what makes more sense ‚Äî here we use issue reports
    combined_view_state = pdk.ViewState(
        latitude=filtered_data['lat'].mean(),
        longitude=filtered_data['lon'].mean(),
        zoom=12,
        pitch=0
    )

    # Combine tooltips using a shared tooltip or leave as-is (each layer can have its own)
    layers_to_show = [layer_reports]
    if show_schools:
        layers_to_show.append(layer_schools)

    r = pdk.Deck(
        layers=layers_to_show,
        initial_view_state=combined_view_state,
        map_style='mapbox://styles/mapbox/dark-v9',
        tooltip=tooltip,
        width=1600,
        height=1000,
    )
    # Show in Streamlit
    st.pydeck_chart(r, use_container_width=True)

    #--------------------------------- DE part ------------------------------------
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
    with open("all_data.json", "r", encoding="utf-8") as f:
        data = json.load(f)

    df = pd.DataFrame(data)

    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏ß‡∏•‡∏≤ title_state
    def parse_duration(text):
        if not isinstance(text, str): return None
        if not text.startswith("‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡πÉ‡∏ô"): return None
        day_match = re.search(r'(\d+)\s*‡∏ß‡∏±‡∏ô', text)
        hr_match = re.search(r'(\d+):(\d+)\s*‡∏ä‡∏°\.|(\d+)\s*‡∏ä‡∏°\.', text)

        total_hours = 0
        if day_match:
            total_hours += int(day_match.group(1)) * 24
        if hr_match:
            if hr_match.group(1) and hr_match.group(2):
                total_hours += int(hr_match.group(1)) + int(hr_match.group(2)) / 60
            elif hr_match.group(3):
                total_hours += int(hr_match.group(3))

        return total_hours if total_hours > 0 else None

    # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï
    def extract_district(text):
        if isinstance(text, str):
            match = re.search(r"‡πÄ‡∏Ç‡∏ï(\S+)", text)
            if match:
                return match.group(1)
        return None

    df["duration_hr"] = df["title_state"].apply(parse_duration)
    df["district"] = df["location_thailand"].apply(extract_district)
    df_clean = df.dropna(subset=["duration_hr", "district"])

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á avg ‡∏ï‡πà‡∏≠‡πÄ‡∏Ç‡∏ï
    df_avg = df_clean.groupby("district")["duration_hr"].mean().reset_index()
    df_avg.columns = ["district", "avg_duration_hr"]
    df_avg["dname"] = "‡πÄ‡∏Ç‡∏ï" + df_avg["district"]

    # ‡πÇ‡∏´‡∏•‡∏î GeoJSON ‡πÄ‡∏Ç‡∏ï 
    geojson_url = "https://raw.githubusercontent.com/pcrete/gsvloader-demo/master/geojson/Bangkok-districts.geojson"
    geojson_path = "bangkok_districts.geojson"

    # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    @st.cache_data
    def load_geojson():
        r = requests.get(geojson_url)
        with open(geojson_path, "wb") as f:
            f.write(r.content)
        with open(geojson_path, "r", encoding="utf-8") as f:
            return json.load(f)

    bangkok_geojson = load_geojson()
    # ‡πÄ‡∏ï‡∏¥‡∏° avg_duration_hr ‡∏•‡∏á‡πÉ‡∏ô GeoJSON
    duration_map = dict(zip(df_avg["dname"], df_avg["avg_duration_hr"]))

    for feature in bangkok_geojson["features"]:
        dname = feature["properties"]["dname"]
        feature["properties"]["avg_duration_hr"] = round(duration_map.get(dname, 0), 2) if dname in duration_map else None


    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà
    m = folium.Map(location=[13.7563, 100.5018], zoom_start=11, width="100%", height="600px")

    Choropleth(
        geo_data=bangkok_geojson,
        data=df_avg,
        columns=["dname", "avg_duration_hr"],
        key_on="feature.properties.dname",
        fill_color="YlOrRd",
        fill_opacity=0.7,
        line_opacity=0.3,
        legend_name="‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)",
        threshold_scale=[0, 5, 10, 15, 20, 25, 30]
    ).add_to(m)

    folium.GeoJson(
        bangkok_geojson,
        style_function=lambda feature: {
            "fillColor": "black" if feature["properties"]["dname"] not in df_avg["dname"].values else "transparent",
            "color": "gray",
            "weight": 0.5,
            "fillOpacity": 0.5
        },
        tooltip=folium.GeoJsonTooltip(
        fields=["dname", "avg_duration_hr"],
        aliases=["‡πÄ‡∏Ç‡∏ï:", "‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (‡∏ä‡∏°.):"],
        localize=True
    )
    ).add_to(m)


    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    st.header("Average Time to Resolve Issues by District")
    m.save("map.html")
    with open("map.html", "r", encoding="utf-8") as f:
        map_html = f.read()

    components.html(map_html, height=650, scrolling=False)





